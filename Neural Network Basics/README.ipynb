{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "Logistic Regreesion is an algorithm for binary classification\n",
    "\n",
    "## Standard Notations for Logistics Regression\n",
    "A single training example is represented by a pair, like $(x,y)$ where $x \\in \\mathcal{R}^{n_x}$ (x is an $\\mathcal{R}^{n_x}$) dimensional feature vector) and $n_x$ is the input size. $y \\in (0,1)$ is the predicted output vector. \n",
    "\n",
    "$m$ denotes the examples in the dataset: $(x^{(1)}, y^{(1)}, x^{(2)},y^{(2)}, \\dots , x^{(m)},y^{(m)})$ are the training examples.\n",
    "\n",
    "$X \\in \\R^{n_x \\times m}$ is the input matrix. Here $`n_x`$ is the number of columns and $m$ is the number of rows.\n",
    "\n",
    "$Y \\in R^{1 \\times m}$ is the label matrix.\n",
    "\n",
    "## Logistic Regression\n",
    "\n",
    "Given $x$, we want to find $\\hat{y}$ where $\\hat{y} = P(y=1|x)$.\n",
    "\n",
    "Parameters: $w \\in \\R^{n_x},\\ b \\in \\R$\n",
    "\n",
    "Output $\\hat{y} = \\sigma (w^T + b)$. We can also write $z = w^T x + b$ where z is the sigmoid function. The sigmoid function is represented as $\\sigma(z) = \\frac{1}{1+e^{-x}}$. \n",
    "\n",
    "If z is large, then $\\sigma(z) \\approx 1$.\n",
    "\n",
    "If z is large negative number, then $\\sigma(z) \\approx 0$.\n",
    "\n",
    "## Logistic Regression Cost Function \n",
    "\n",
    "We have $\\hat{y} = \\sigma(w^Tx+b)$, where $\\sigma (z) = \\frac{1}{1+1+e^{-z}}$.\n",
    "\n",
    "Suppose we are given the training set: $`\\{(x^{(1)}, y^{(1)}), \\dots, (x^{(m)}, y^{(m)}) \\}`$ and we want $`\\hat{y}^{(i)} = y^{(i)} `$. We will use the cost function to analysis this approximation. The cost function of Logsitic Regression is given by:\n",
    "$` L(\\hat{y}, y)  = -(y \\text{log} \\hat{y})  + (1-y) \\text{log}(1-\\hat{y}) `$\n",
    "\n",
    "If $`y=1:\\  L(\\hat{y}, y)  = -(\\text{log} \\hat{{y}}) `$\n",
    "\n",
    "If $`y=0:\\ L(\\hat{y}, y)  = (1-y) \\text{log}(1-\\hat{y}) `$\n",
    "\n",
    "Toghther, it becomes: \n",
    "\n",
    "$$\n",
    "J(w,b) = \\frac{1}{m} \\sum_{i=1}^{m} L(\\hat{y}^{(i)}, y^{(i)} ) = -\\frac{1}{m} \\sum_{i=1}^{m} \\big[ y^{(i)} \\text{log} \\hat{y}^{(i)} + (1-y^{(i)}) \\text{log}(1 - \\hat{y}^{(i)}  ) \\big]\n",
    "$$\n",
    "\n",
    "## Gradient Descent\n",
    "\n",
    "Check the Data Sciene Toolkit to learn more about usage of Gradient Descent.\n",
    "\n",
    "After this, we would like to implement the gradient descent on m training examples.\n",
    "\n",
    "We have the cost function\n",
    "$`J(w,b) = \\frac{1}{m} \\sum_{i=1}^{m} L(a^{(i)} , y^{(i)}) `$, where $`a^{(i)} = \\hat{y}^{(i)} = \\sigma (w^Tx^{(i)} + b) `$\n",
    "\n",
    "Derivating the cost function w.r.t $`w_1`$, we get \n",
    "\n",
    "$`\\frac{\\partial}{\\partial w_1}\\ J(w, b) = \\frac{1}{m} \\sum_{1}^{m} \\sum_{i=1}^{m} \\frac{\\partial}{\\partial} L(a^{(i)}, y^{(i)})`$\n",
    "\n",
    "\n",
    "\n",
    "## Algorithm for Gradient Descent\n",
    "\n",
    "Initialize $`J = 0,\\ dw_1 = 0,\\ dw_2 = 0,\\ db = 0`$.\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\text{For } i = 1 \\text{ to } & m\\\\\n",
    "z^{(i)} &= w^T x^{(i)} + b\\\\\n",
    "a^{(i)} &= \\sigma(z^{(i)})\\\\\n",
    "J+ & = -[ y^{(i)} \\text{log}a^{(i)} + (1-y^{(i)}) \\text{log}(1-a^{(i)}) ]\\\\\n",
    "dz^{(i)} & = a^{(i)} - y^{(i)}\\\\\n",
    "dw_1 &+= x^{(i)} dz^{(i)}\\\\\n",
    "\\vdots\\\\\n",
    "dw_n &+= x^{(i)} dz^{(i)}\\\\\n",
    "db &+= dz^{(i)}\\\\\n",
    "J = J/m& \\\\\n",
    "dw_1 = dw_1&/m ;\\ dw_2 = dw_2/m;\\ \\dots dw_n = dw_n/m ; db = db/m\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Writing a for loop in Python is not considered a good pracitce if we are performing a mathematical calculation which involves a heavy computation. So in our case, instead of writing,\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "dw_1 &+= x^{(i)} dz^{(i)}\\\\\n",
    "\\vdots\\\\\n",
    "dw_n &+= x^{(i)} dz^{(i)}\\\\\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "in the for loop, we will use the *np.dot* function from the numpy library to perform the computations. So, we are going to get rid of $`dw_1 =0 `$ and $`dw_2 = 0`$, we are going to initialize $`dw`$ by a vector as *dw = np.zeros($`n_x`$, 1)*.\n",
    "\n",
    "### Vectorizing the Gradient Descent Algorithm\n",
    "\n",
    "Instead of the for loop we are going to use the vector value operation as *dw +=$x^{(i)}$ dz^{(i)}*\n",
    "\n",
    "and we will also get rid of the last line in the algorithm $`dw_1 = dw_1/m ;\\ dw_2 = dw_2/m;\\ \\dots dw_n = dw_n/m `$ and replace it with $`dw / = m`$\n",
    "\n",
    "Now we are also going to elliminate the first for loop that we have. Instead we are going to use the numpy arrays to carry out the mathematical operations.\n",
    "\n",
    "So $`X \\in \\R^{n_x \\times m}`$ matrix.\n",
    "\n",
    "$`w^T = \\R^{(1, n_x)}`$ will be row vector.\n",
    "\n",
    "$`z = [z^{(1)}\\ \\dots z^{(m)}]` = w^T X [b \\dots b]  = [w^T x^{(1)}+ b \\ w^T x^{(2)}+b \\dots w^T x^{(m)} +b ]`$\n",
    "\n",
    "Thus $`z = \\text{np.dot(w.T, x) + b}`$ in Python.\n",
    "\n",
    "At last, $`A = [a^{(1)} \\ a^{(2)} \\dots a^{(m)} ] = \\sigma(z) `$\n",
    "\n",
    "## Vectorizing Logsitic Regression\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
