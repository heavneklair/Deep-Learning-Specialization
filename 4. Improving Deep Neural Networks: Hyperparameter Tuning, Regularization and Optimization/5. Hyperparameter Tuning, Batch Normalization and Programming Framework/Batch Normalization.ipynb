{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalization\n",
    "\n",
    "## Normalizing activations in a Network\n",
    "\n",
    "Batch normalization makes the hyperparameters search problem much easier, make the neural network more robust to the choice of hyperparameters, and enables us to much more easily train neural nets (even deep)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we have a neural network of 3 hidden layers. The idea behind normalization is that: wouldn't it be nice if we can normalize the mean and variance of $a^{[2]}$ to make the training of $W^{[3]},\\ b^{[3]}$ more efficient? Can we even do this? \n",
    "\n",
    "The answer is almost yes because actually we normalize the values of $z^{[2]}$ which will give us the same effect. It is quite not sure if we should normalize the value before the acitivation function, $a^{[2]}$ or after? In practice, normalization of $z^{[2]}$ is done much more often, and that is what we will discuss here.\n",
    "\n",
    "**Implement Batch Norm**\n",
    "Give some intermediate values in Neural Network: $z^{[l](i)}$ for a hidden layer $l$ and $i-th$ example. We will compute the mean and variance as follows: \n",
    "$$ \\mu = \\frac{1}{m} \\sum_{i=1}^{m} z^{(i)} $$\n",
    "$$ \\sigma^2 = \\frac{1}{m} \\sum_{i=1}^{m} (z_i - \\mu)^2 $$ \n",
    "Then we will normalize them as:\n",
    "$$ z_{\\text{norm}}^{(i)} = \\frac{ z^{(i)} - \\mu }{ \\sqrt{\\sigma^2 + \\epsilon}} $$\n",
    "\n",
    "We do not want the hidden units to always have mean 0 and variance 1 because the hidden units have a different distribution. So what we will do instead is: \n",
    "\n",
    "$$ \\tilde{z}^{(i)} = \\gamma z_{\\text{norm}}^{(i)} + \\beta $$\n",
    "where $\\gamma$ and $\\beta$ are learnable parameters of your model.\n",
    "\n",
    "While using any model of Gradient Descent, we will update the parameters ($z's$) just as we would update the weights of the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting Batch Norm into a Neural Network\n",
    "\n",
    "Here is how we can add batch norm to a network. This is just considering one unit ofthe 2 hidden layer neural network\n",
    "$$ X \\overset{w^{[1]}, b^{[1]} }{\\longrightarrow} z^{[1]}   \\overset{ \\beta^{[1]}, \\gamma^{[1]} }{\\underset{\\text{Batch Norm}} {\\longrightarrow}} \\tilde{z}^{[1]}  \\longrightarrow a^{[1]} = g^{[1]}(\\tilde{z}^{[1]}) \\overset{w^{[1]}, b^{[1]} }{\\longrightarrow} z^{[1]} \\overset{ \\beta^{[2]}, \\gamma^{[2]} }{\\underset{\\text{BN}} {\\longrightarrow}} \\tilde{z}^{[2]} \\longrightarrow a^{[2]} \\ldots $$ \n",
    "\n",
    "Similarly for the other layers.\n",
    "\n",
    "The parameters of the network are going to be: \n",
    "$W^{[1]},\\ b^{[1]}, \\ldots \\ , W^{[L]},\\ W^{[L]}, \\beta^{[1]},\\ \\gamma^{[1]},\\ \\ldots \\beta^{[L]},\\ \\gamma^{[L]}$\n",
    "\n",
    "These $\\beta$'s have nothing to do with the hyperparameter $\\beta$ that we had for momentum over computing the various exponentially weighted averages. If we are using a Deep Learning Programming Framework, we do not need to implement the Batch-norml layer. The Framework will do that for us. \n",
    "\n",
    "**NOTE** In practice, Batch Norm is usually applied with mini-batches of the training set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
