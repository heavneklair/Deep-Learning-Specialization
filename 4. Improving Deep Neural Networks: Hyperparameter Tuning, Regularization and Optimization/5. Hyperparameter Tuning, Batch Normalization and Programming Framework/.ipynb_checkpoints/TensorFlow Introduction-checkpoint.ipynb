{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a motivation, lets say that we have a to minimize the following cost function\n",
    "$$  J(w)  = w^2 -10 w + 25 $$\n",
    "We can notice that this function is an expanded form of $(w-5)^2$. Thus we can tell from this form that the minimum will achieve at $w=5$. We will use TensorFlow to predict this result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will define the parameter $w$. We are going to use `tf.variable()` to signify that this is a variable initialized to 0 and the type of the variable `dtype` is floating point number, `tf.float32`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.0>\n"
     ]
    }
   ],
   "source": [
    "w = tf.Variable(0, dtype = tf.float32)\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we are going to define the optimization algorithm that we are going to use. In this case, the Adam optimization algorithm and then we are going to define the cost function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(0.1)  # alpha = 0.1\n",
    "\n",
    "# cost = w**2 - 10*w + 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only need to implement the forward prop i.e. we only need to write the function to compute the value of the cost function. Tensorflow can figure out how to do the Backprop or the gradient computation. \n",
    "\n",
    "- One way to do that is to use `GradientTape`. The `GradientTape` is the analogy to the old school cassette tape where it will record the sequence of operations as we are computing the cost function in the ForwardProp step. When we play the tape backward, in backward order, it can revisit the order of operations in reverse order and along the way, it can compute backprop and the gradients.\n",
    "\n",
    "- We also need to define a training step function, `train-step()` to loop over. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step():\n",
    "    with tf.GradientTape() as tape:\n",
    "        cost = w**2 - 10*w + 25\n",
    "        \n",
    "    trainable_variables = [w]\n",
    "    grads = tape.gradient(cost, trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, trainable_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In order to carry out the one iteration of training, we have to define what are the trainable variables, `trainable_variables`. It is just a list with only $w$.\n",
    "- Then we are going to compute the gradients, `grads` with `tape.gradient()`.\n",
    "- Having all these steps, we can now use the optimizer to apply the gradients with `apply_gradients()`. We are going to use the `zip` function to take the list of the graidents and trainable_variables, and pair them up. So a `zip` function take the given parameters and pairs the corresponding elements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's run one step of our algorithm and print the new value of $w$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.09999997>\n"
     ]
    }
   ],
   "source": [
    "train_step()\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the value of $w$ has changed from 0 to 0.9999. Let's run a 1000 iterations of the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=5.000001>\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    train_step()\n",
    "    \n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the value has now changed to 5, which is the minimum value for the cost function. So we just specified the cost function and Tensorflow has computed the minimum value for us. In other words, it has minimized the cost function for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With respect to Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, we just had $w$ as a fixed parameter or the variable in the cost function. What if we the function we want to minimize is not just a function of $w$, but also a function of your training step? Let us say that we have some data, $x$ and $y$, and we are training a neural network with the cost function which depends upon the data. How do we get the training data into a Tensorflow program? \n",
    "\n",
    "Let's use the same $w$ and the optimizer with Adam's Algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = tf.Variable(0, dtype = tf.float32)\n",
    "x = np.array([1.0, -10.0, 25.0], dtype =np.float32)\n",
    "optimizer = tf.keras.optimizers.Adam(0.1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, the numbers in the array will play the role of the coefficients of the cost function. \n",
    "\n",
    "Now, lets define the cost function which will minimize the same as before except that now, we are going to write  the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.09999997>\n"
     ]
    }
   ],
   "source": [
    "def cost_fn():\n",
    "    return x[0]* w**2 + x[1]*w + x[2]\n",
    "\n",
    "optimizer.minimize(cost_fn, [w])\n",
    "print(w)\n",
    "\n",
    "# With this we ran the algorithm once (one-step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `optimizer.minize(cost_fn, [w])` is a simpler version (does the same thing) in comparison to the three lines of code we wrote above (with the `GradientTape`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=5.000001>\n"
     ]
    }
   ],
   "source": [
    "w = tf.Variable(0, dtype = tf.float32)\n",
    "x = np.array([1.0, -10.0, 25.0], dtype =np.float32)\n",
    "optimizer = tf.keras.optimizers.Adam(0.1) \n",
    "\n",
    "def training(x, w, optimizer):\n",
    "    def cost_fn():\n",
    "        return x[0]* (w**2) + x[1]*w + x[2]\n",
    "    for i in range(1000):\n",
    "        optimizer.minimize(cost_fn, [w])\n",
    "    return w\n",
    "\n",
    "w = training(x,w, optimizer)\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, $w$ achieves the minimum like before."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
