{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Neural Network Notation\n",
    "* L = number of layers\n",
    "* $n^{[l]}$ = number of units in layers, $l$\n",
    "    - $n^{[0]} = n_x = $ number of units in the input layer\n",
    "* $a^{[l]}$ = activations in layer $l$\n",
    "    - we compute $a^{[l]} = g^{[l]}(z^{[l]})$\n",
    "* $w^{[l]} = $weights for computing $z^{[l]}$ in layer $l$\n",
    "* $b^{[l]} = $ used to compute $z^{[l]}$\n",
    "* Input features are called $X$ and $X = a^{[0]}$. The activation of the final layer, $a^{[l]} = \\hat{y}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation in a Deep Neural Network\n",
    "\n",
    "The general equation for forward propagation for a single training example looks like:\n",
    "$$ \n",
    "z^{[l]} = w^{[l]} a^{[l-1]} + b^{[l]}\\\\\n",
    "a^{[l]} = g^{[l]}(z^{[l]})\n",
    "$$\n",
    "\n",
    "The general equation for forward propagation for vectorized sytem of equations of the entire training set looks like:\n",
    "$$ \n",
    "Z^{[l]} = W^{[l]} A^{[l-1]} + ^{[l]}\\\\\n",
    "A^{[l]} = g^{[l]}(Z^{[l]})\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the matrix dimensions right\n",
    "\n",
    "- The dimensions of parameter $W^{[l]}$ are: $(n^{[l]}, n^{[l-1]})$.\n",
    "- The dimensions of parameter $b^{[l]}$ are: $(n^{[l]}, 1)$.\n",
    "- If we are implementing the backward propagation, the dimensions of $dW^{[l]}$ are: $(n^{[l]},  n^{[l-1]}))$ (same dimensions as $W^{[l]})$\n",
    "- If we are implementing the backward propagation, the dimensions of $db^{[l]}$ are: $(n^{[l]}, 1)$ (same dimensions as $b^{[l]})$\n",
    "\n",
    "Now lets check the dimensions of $a^{[l]},\\ z^{[l]}$ and  $A^{[l]},\\ Z^{[l]}$.\n",
    "\n",
    "We know that equation\n",
    "$$ \n",
    "\\underbrace{z^{[l]}}_{(n^{[l]}, 1)} = \\underbrace{w^{[l]}}_{(n^{[l]}, 1)} \\cdot \\underbrace{x}_{(n^{[0]}, 1)} = \\underbrace{b^{[l]}}_{(n^{[1]}, 1)}\n",
    "$$\n",
    "\n",
    "In the vectorized form, it beomes\n",
    "$$\n",
    "\\underbrace{Z^{[l]}}_{(n^{[l]}, m)} = \\underbrace{W^{[l]}}_{(n^{[l]}, n^{[l-1]})} \\cdot \\underbrace{X}_{(n^{[l-1]}, m)} = \\underbrace{b^{[l]}}_{(n^{[1]}, 1)}\n",
    "$$\n",
    "- Thus dimension of ${Z^{[l]}}$ and $A^{[l]}$ are $(n^{[l]}, m)$.\n",
    "- The dimensions of $A^{[0]} = X$ are $(n^{[0]}, m)$\n",
    "- If we are implementing the backward propagation, the dimensions of $dZ^{[l]},\\ dA^{[l]}$ are: $(n^{[l]}, m)$ (same dimensions as $Z^{[l]})$ and $A^{[l]}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Blocks of Deep Neural Network\n",
    "\n",
    "### Forward and Backward Propagation\n",
    "\n",
    "Suppose we are at the layer $l$ in the network. Then we have: \n",
    "- $W^{[l]},\\ b^{[l]}$\n",
    "- For forward Propagation: Input $a^{[l-1]}$, $\\quad$ Output $a^{[l]}$\n",
    "- For backward Propagation, Input $da^{[l]}$, cache$(z^{[l]})$, $\\qquad$ ouput $da^{[l-1]}$, $dw^{[l]}$, $db^{[l]}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward and Forward Propagation\n",
    "\n",
    "### Forward Propagation for layer $l$\n",
    "\n",
    "- Input $a^{[l-1]}$\n",
    "- Output $a^{[l]}$, cache($z^{[l]}$, along with $w^{[l]}$, and $b^{[l]}$)\n",
    "\n",
    "Implentations for single training examples\n",
    "$$ \n",
    "z^{[l]} = w^{[l]} \\cdot a^{[l-1]} + b^{[l]}\\\\\n",
    "a^{[l]} = g^{[l]}(z^{[l]})\n",
    "$$\n",
    "\n",
    "and in Vecotrized Form\n",
    "$$\n",
    "Z^{[l]} = W^{[l]} \\cdot A^{[l-1]} + b^{[l]}\\\\\n",
    "A^{[l]} = g^{[l]}(Z^{[l]})\n",
    "$$\n",
    "\n",
    "### Backward Propagation for layer $l$\n",
    "\n",
    "- Input $da^{[l]}$\n",
    "- Output $da^{[l-1]},\\ dW^{[l]},\\ db^{[l]}$ along with $a^{[l-1]}$  in cache\n",
    "\n",
    "Implentations for single training examples\n",
    "$$ \n",
    "dz^{[l]} = da^{[l]} \\times g^{[l]'}(z^{[l]})\\\\\n",
    "dW^{[l]} = dz^{[l]} \\cdot a^{[l-1]}\\\\\n",
    "db^{[l]} = dz^{[l]} \\cdot a^{[l-1]}\\\\\n",
    "da^{[l-1]} = W^{[l]T} dz^{[l]}\\\\\n",
    "\\text{ Repeat from above }\n",
    "$$\n",
    "\n",
    "and in Vecotrized Form\n",
    "$$\n",
    "dZ^{[l]} = dA^{[l]} \\times g^{[l]'}(Z^{[l]})\\\\\n",
    "dW^{[l]} = \\frac{1}{m} dZ^{[l]} \\cdot A^{[l-1]T}\\\\\n",
    "db^{[l]} = \\frac{1}{m} \\text{ np.sum } ( dZ^{[l]} , \\text{ axis = 1}, \\text{ keepdims =True})\\\\\n",
    "dA^{[l-1]} = W^{[l]T} dZ^{[l]}\\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters vs Hyperparameters\n",
    "\n",
    "Parameters: $W^{[l]},\\ b^{[l]}, \\dots$\n",
    "\n",
    "Hyperparameters: \n",
    "- Learning rate $\\alpha$\n",
    "- Number of iterations\n",
    "- Number of hidden layer ($l$)\n",
    "- Hidden units $n^{[1]},\\ n^{[2]}, \\dots$\n",
    "- Choice of activation function\n",
    "\n",
    "Later, we will also see momentum term, mini batch size, various forms of regularization parameters, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
