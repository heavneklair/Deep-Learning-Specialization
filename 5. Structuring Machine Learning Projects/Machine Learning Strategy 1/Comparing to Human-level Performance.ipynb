{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why Human-level Performance\n",
    "Recently a lot of machine learning teams have been talking about comparing the machine learning systems to human level performance. Two reasons for this are: \n",
    "- Because of advances in deep learning, machine learning algorithms are suddenly working much better and so it has become much feasible in a lot of application areas for machine learning algorithms to actually become comptetitive with human-level performance.\n",
    "- THe worflow of designing and building a machine learning system is much more efficient when we are trying to do something than humans can do.\n",
    "\n",
    "The progress of a machine learning sytem progresses rapidly as we approch human-level performance but slows down when the algorithm passes it until it achieves a theoretical optimum level of performance. This theoretical limit is called Bayes optimal error. (In other words, it is the best possible error). \n",
    "\n",
    "For tasks that humans are good at, so long as your machine learning algorithm is still worse than human, you can get labeled data from humans."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aviodable Bias\n",
    "\n",
    "Let us suppose the following: \n",
    "- Human Error - 1% \n",
    "- Training Error - 8% \n",
    "- Dev Error - 10% \n",
    "\n",
    "Since there is a lot of difference between Human Error and Training error, we can conclude that the algorithm is even fitting the data well. So in terms of reducing bias or variance, we will focus on **reducing bias**. \n",
    "\n",
    "Now let us suppose \n",
    "- Human Error - 7.5% \n",
    "- Training Error - 8% \n",
    "- Dev Error - 10% \n",
    "\n",
    "Here, the gap is bigger between Training and Dev Error, so we will focus on variance. \n",
    "We can think of humnan level error as a proxy or as an estimate for Bayes or for Bayes Optimal error. In computer vision, there isa  pretty reasonable proxy because humans are actually very good at computer vision and so whatever a human can do is not too far from Bayes error. We cannot do better than Bayes error unless we are overfitting. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Human-Level Performanc\n",
    "\n",
    "The gap between Bayes error and training error is called a measure of the avoidable bias. And this as a measure or an estimate of how much of a variance problem you have in your learning algorithm."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2f9be63dc4f67cd23638b8e785a0e27a8af8598712c0c689e320ac3cb92769c8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
